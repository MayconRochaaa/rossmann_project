{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 IMPORTS\n",
    "\n",
    "    Bibliotecas utilizadas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy                as np\n",
    "import pickle\n",
    "import pandas               as pd\n",
    "import xgboost              as xgb\n",
    "import seaborn              as sns\n",
    "import inflection\n",
    "import matplotlib.pyplot    as plt\n",
    "\n",
    "from scipy                      import stats    as ss\n",
    "from boruta                     import BorutaPy\n",
    "from datetime                   import datetime, timedelta\n",
    "from IPython.display            import Image\n",
    "from sklearn.metrics            import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.ensemble           import RandomForestRegressor\n",
    "from sklearn.linear_model       import LinearRegression, Lasso\n",
    "from sklearn.preprocessing      import RobustScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection    import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 FUNCTIONS\n",
    "    \n",
    "    Funções utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def is_promo(row):\n",
    "    if row['promo_interval'] == 0:\n",
    "        return 0\n",
    "    elif row['month_map'] in row['promo_interval'].split(','):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def cramer_v (x,y):\n",
    "\n",
    "    cm = pd.crosstab(x, y).to_numpy()\n",
    "    n=cm.sum()\n",
    "    r,k=cm.shape\n",
    "    \n",
    "    chi2 = ss.chi2_contingency(cm)[0]\n",
    "    \n",
    "    chi2corr = max(0, chi2 - (k-1)*(r-1)/(n-1))\n",
    "    kcorr = k - (k-1)**2/(n-1)\n",
    "    rcorr = r - (r-1)**2/(n-1)\n",
    "    \n",
    "    v = np.sqrt((chi2corr/n)/(min(kcorr-1,rcorr-1)))\n",
    "    \n",
    "    return v\n",
    "\n",
    "def ml_error(model_name, y, yhat):\n",
    "    mae = mean_absolute_error(y,yhat)\n",
    "    mape = mean_absolute_percentage_error(y,yhat)\n",
    "    rmse = np.sqrt(mean_squared_error(y, yhat))\n",
    "    \n",
    "    return pd.DataFrame({'Model Name': model_name,\n",
    "                         'MAE': mae,\n",
    "                         'MAPE': mape,\n",
    "                         'RMSE':rmse }, index=[0])\n",
    "\n",
    "def cross_validation(x_training, kfold, model_name, model, verbose=False):\n",
    "    mae_list=[]\n",
    "    mape_list=[]\n",
    "    rmse_list=[]\n",
    "\n",
    "    for k in reversed(range(1,kfold+1)):\n",
    "        if verbose:\n",
    "            print('\\nKFold Number: {}'.format(k))\n",
    "        #start and end date for validation\n",
    "        validation_start_date = x_training['date'].max()-timedelta(days=6*7*(k))\n",
    "        validation_end_date = x_training['date'].max()-timedelta(days=6*7*(k-1))\n",
    "\n",
    "        # filtering dataset\n",
    "        training = x_training[x_training['date'] < validation_start_date]\n",
    "        validation = x_training[(x_training['date'] >= validation_start_date) & (x_training['date'] <= validation_end_date)]\n",
    "\n",
    "\n",
    "        #\n",
    "        xtraining = training.drop(['date', 'sales'], axis=1)\n",
    "        ytraining = training['sales']\n",
    "\n",
    "        xvalidation = validation.drop(['date', 'sales'], axis=1)\n",
    "        yvalidation = validation['sales']\n",
    "\n",
    "\n",
    "        m = model.fit(xtraining, ytraining)\n",
    "\n",
    "        yhat = m.predict(xvalidation)\n",
    "\n",
    "        m_result = ml_error(model_name, yvalidation, yhat)\n",
    "        mae_list.append(m_result['MAE'])\n",
    "        mape_list.append(m_result['MAPE'])\n",
    "        rmse_list.append(m_result['RMSE'])\n",
    "        \n",
    "    return pd.DataFrame({'Model Name': model_name,\n",
    "                         'MAE CV':  np.round(np.mean(mae_list),2).astype(str) + '+/-' + np.round(np.std(mae_list),2).astype(str),\n",
    "                         'MAPE CV': np.round(np.mean(mape_list),2).astype(str) + '+/-' + np.round(np.std(mape_list),2).astype(str),\n",
    "                         'RMSE CV': np.round(np.mean(rmse_list),2).astype(str) + '+/-' + np.round(np.std(rmse_list),2).astype(str)}, index=[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 DIRETÓRIOS/LOADING DATA\n",
    "\n",
    "    Definição dos diretórios e leitura dos dados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.abspath('')   \n",
    "DATA_DIR = os.path.join(BASE_DIR,'data')\n",
    "PARA_DIR = os.path.join(BASE_DIR,'parameter')\n",
    "\n",
    "df_sales_raw = pd.read_csv(os.path.join(DATA_DIR,'train.csv'), low_memory = False)\n",
    "df_store_raw = pd.read_csv(os.path.join(DATA_DIR,'store.csv'), low_memory = False)\n",
    "df_raw = pd.merge(df_sales_raw, df_store_raw, how='left', on='Store')\n",
    "df1 = df_raw.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 DATA DISCRIPTION\n",
    "\n",
    "    Verificações e alterações básicas no dataframe\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Rename Columns\n",
    "    As colunas são reescritas no modo snake, todas minusculas e com _ no lugar de espaços"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "cols_old = ['Store', 'DayOfWeek', 'Date', 'Sales', 'Customers', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment', 'CompetitionDistance',\n",
    "            'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval']\n",
    "\n",
    "snakecase = lambda x: inflection.underscore(x)\n",
    "cols_new = list(map(snakecase, cols_old))\n",
    "\n",
    "#rename\n",
    "df1.columns = cols_new"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Dimensions / Types\n",
    "    Dimensões do dataframe de treino e os tipos de colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "print('Number of Rows: {}'.format(df1.shape[0]))\n",
    "print('Number of Columns: {}'.format(df1.shape[1]))\n",
    "df1.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Check // Fillout NA\n",
    "    Verificação das colunas e quantidade de linhas que possuem NaN. Esses valores faltantes são preenchidos com base na lógica do negócio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.isna().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preenchimento dos valores NaN\n",
    "###### Todas presunções assumidass são baseadas nos meus conhecimentos em problemas de negócio e não necessáriamente refletem a realidade das lojas Rossmann\n",
    "\n",
    "##### competition_distance\n",
    "    Os valores não preenchidos nessa coluna provavelmte significam que essas lojas não possuem um competidor próximo, portanto, para preencher essas coluna utilizarei valores de distancias muito grandes, no caso 200000 metros.\n",
    "\n",
    "##### competition_open_since_month\n",
    "    Aqui temos o tempo, por mês, em que a ultima loja concorrente abriu. Os valores NaN se devem provavelmente por lojas não possuem competidor perto, ou que o competidor já existia no momento em que a loja da Rossmann foi inaugurada, ou ainda por simplesmente por falta de acesso a essa informação. Vamos considerar então a data da venda como essa data, dessa forma a diferença entre a data de venda e abertura será zero para os valores NaN.\n",
    "\n",
    "##### competition_open_since_year // promo2_since_week // promo2_since_year\n",
    "    Aplicaremos a mesma lógica do caso anterior\n",
    "\n",
    "##### promo_interval\n",
    "    0 se não houve promoção\n",
    "\n",
    "Além disso também foi alterado os meses de números para seus nomes abreviados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "df1['date'] = pd.to_datetime(df1['date'])\n",
    "# competition_distance\n",
    "df1['competition_distance'].fillna(200000.0, inplace=True)\n",
    "\n",
    "# competition_open_since_month\n",
    "df1['competition_open_since_month'] = df1.apply(lambda x: x['date'].month if math.isnan(x['competition_open_since_month']) else x['competition_open_since_month'], axis=1)\n",
    "\n",
    "# competition_open_since_year\n",
    "df1['competition_open_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['competition_open_since_year']) else x['competition_open_since_year'], axis=1)\n",
    "\n",
    "# promo2_since_week\n",
    "df1['promo2_since_week'] = df1.apply(lambda x: x['date'].week if math.isnan(x['promo2_since_week']) else x['promo2_since_week'], axis=1)\n",
    "\n",
    "# promo2_since_year\n",
    "df1['promo2_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['promo2_since_year']) else x['promo2_since_year'], axis=1)\n",
    "\n",
    "# promo_interval\n",
    "df1['promo_interval'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "month_map = {1: 'Jan', 2: 'Fev', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', 7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'}\n",
    "df1['month_map'] = df1['date'].dt.month.map(month_map)\n",
    "df1['is_promo'] = df1.apply(is_promo, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Change Types\n",
    "    Transformação nos tipos de variaveis das colunas, se necessário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "df1['competition_open_since_month'] = df1['competition_open_since_month'].astype(int)\n",
    "df1['competition_open_since_year'] = df1['competition_open_since_year'].astype(int)\n",
    "df1['promo2_since_week'] = df1['promo2_since_week'].astype(int)\n",
    "df1['promo2_since_year'] = df1['promo2_since_year'].astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Descriptive Statistical\n",
    "    Analise estatistica descritiva das variaveis. Para as variaveis numericas foram tomadas medidas de tendencia central (média e mediana) e medidas de dispersão (desviopadrão, range, skewness e kurtosis). Para as váriaveis categóricas foi realizado um boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "num_attributes = df1.select_dtypes(include=['int64', 'float64'])\n",
    "cat_attributes = df1.select_dtypes(exclude=['int64', 'float64', 'datetime64[ns]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "#Central tendency - mean, median\n",
    "ct1 = pd.DataFrame(num_attributes.apply(np.mean)).T\n",
    "ct2 = pd.DataFrame(num_attributes.apply(np.median)).T\n",
    "\n",
    "#Dispersion - std, min, max, range, skew, kurtosis\n",
    "d1 = pd.DataFrame(num_attributes.apply(np.std)).T\n",
    "d2 = pd.DataFrame(num_attributes.apply(min)).T\n",
    "d3 = pd.DataFrame(num_attributes.apply(max)).T\n",
    "d4 = pd.DataFrame(num_attributes.apply(lambda x: x.max()-x.min())).T\n",
    "d5 = pd.DataFrame(num_attributes.apply(lambda x: x.skew())).T\n",
    "d6 = pd.DataFrame(num_attributes.apply(lambda x: x.kurtosis())).T\n",
    "\n",
    "m = pd.concat([d2, d3, d4, ct1, ct2, d1, d5, d6]).T.reset_index()\n",
    "\n",
    "m.columns = (['attributes', 'min', 'max', 'range', 'mean', 'median', 'std', 'skew', 'kurtosis'])\n",
    "\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "cat_attributes.apply(lambda x: x.unique().shape[0])\n",
    "\n",
    "aux1 = df1[(df1['state_holiday'] != '0') & (df1['sales'] > 0)]\n",
    "\n",
    "fig = plt.subplots(1, 3, figsize=(21, 7))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "sns.boxplot(x='state_holiday', y='sales', data=aux1)\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.boxplot(x='store_type', y='sales', data=aux1)\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.boxplot(x='assortment', y='sales', data=aux1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 FEATURE ENGINEERING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "df2 = df1.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Mapa mental das hipoteses\n",
    "    Para dar inicio ao processo de feature engineering foi criado um mapa mental de hipoteses, onde, me baseando no problema de negócio estabelecido, criei diferentes hipoteses separadas por Lojas, Clientes, Produtos, Localização e Sazonalidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('images/mind_map_hipoteses.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Criação das hipoteses\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hipoteses  loja\n",
    "\n",
    "**1.** Lojas com maior quadro de funcionarios deveriam vender mais;\n",
    "\n",
    "**2.** Lojas com maior estoque deveriam vender mais;\n",
    "\n",
    "**3.** Lojas com maior porte deveriam vender mais;\n",
    "\n",
    "**4.** Lojas com menor porte deveriam vender menos;\n",
    "\n",
    "**5.** Lojas com maior sortimento deveriam vender mais;\n",
    "\n",
    "**6.** Lojas com competidores mais próximos deveriam vender menos;\n",
    "\n",
    "**7.** Lojas com competidores à mais tempo deveriam vender mais."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hipoteses do produto\n",
    "\n",
    "**1.** Lojas que investem mais em marketing deveriam vender mais;\n",
    "\n",
    "**2.** Lojas que expoem mais os produtos na vitrine deveriam vender mais;\n",
    "\n",
    "**3.** Lojas que tem preços menores nos produtos deveriam vender mais;\n",
    "\n",
    "**4.** Lojas que tem preços menores por mais tempo nos produtos deveriam vender mais.\n",
    "\n",
    "**5.** Lojas com promoções mais agressivais deveriam vender mais;\n",
    "\n",
    "**6.** Lojas com mais dias de promoção deveriam vender mais;\n",
    "\n",
    "**7.** Lojas com mais promoções consecutivas deveriam vender mais."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hipoteses de tempo\n",
    "\n",
    "**1.** Lojas que abrem durante o natal deveriam vender mais;\n",
    "\n",
    "**2.** Lojas deveriam vender mais ao longo dos anos;\n",
    "\n",
    "**3.** Lojas deveriam vender mais no segundo semestre do ano;\n",
    "\n",
    "**4.** Lojas deveria vender mais após o dia 10 de cada mês;\n",
    "\n",
    "**5.** Lojas deveriam vender menos aos finais de semana;\n",
    "\n",
    "**6.** Lojas deveriam vender menos durante feriados escolares;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lista final de hipoteses\n",
    "\n",
    "    Nem todas hipóteses anteriores podem ser respondidas com o dataset disponibilizado, devido a isso seleceionei apenas algumas:\n",
    "\n",
    "**1.** Lojas com maior sortimento deveriam vender mais;\n",
    "\n",
    "**2.** Lojas com competidores mais próximos deveriam vender menos;\n",
    "\n",
    "**3.** Lojas com competidores à mais tempo deveriam vender mais.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "**4.** Lojas que tem preços menores por mais tempo nos produtos deveriam vender mais.\n",
    "\n",
    "**5.** Lojas com mais dias de promoção deveriam vender mais;\n",
    "\n",
    "**6.** Lojas com mais promoções consecutivas deveriam vender mais.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "**7.** Lojas que abrem durante o natal deveriam vender mais;\n",
    "\n",
    "**8.** Lojas deveriam vender mais ao longo dos anos;\n",
    "\n",
    "**9.** Lojas deveriam vender mais no segundo semestre do ano;\n",
    "\n",
    "**10.** Lojas deveria vender mais após o dia 10 de cada mês;\n",
    "\n",
    "**11.** Lojas deveriam vender menos aos finais de semana;\n",
    "\n",
    "**12.** Lojas deveriam vender menos durante feriados escolares;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Feature Engineering\n",
    "    A partir de agora começa o processo de extração de novas variavéis a partir das disponiveis no dataframe. As variaveis derivadas abaixo são, respectivamente:\n",
    "    - Ano\n",
    "    - Mês\n",
    "    - Dia\n",
    "    - Semana do ano\n",
    "    - Ano-Semana\n",
    "    - Tempo de competição\n",
    "    - Tempo de competição por mês\n",
    "    - Tempo de promoção\n",
    "    - Tempo de promoção por semana\n",
    "    - Alteração nos valores de assortment/state_holiday para seus devidos significados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "#Year\n",
    "df2['year'] = df2['date'].dt.year\n",
    "\n",
    "#Month\n",
    "df2['month'] = df2['date'].dt.month\n",
    "\n",
    "#Day\n",
    "df2['day'] = df2['date'].dt.day\n",
    "\n",
    "#Week of Year\n",
    "df2['week_of_year'] = df2['date'].dt.isocalendar().week\n",
    "\n",
    "#Year week\n",
    "df2['year_week'] = df2['date'].dt.strftime('%Y-%W')\n",
    "\n",
    "\n",
    "#competition since\n",
    "df2['competition_since'] = df2.apply(lambda x: datetime(year=x['competition_open_since_year'], month=x['competition_open_since_month'],day=1),axis=1)\n",
    "df2['competition_time_month'] = ((df2['date']-df2['competition_since'])/30).apply(lambda x: x.days).astype(int)\n",
    "\n",
    "#Promo since\n",
    "df2['promo_since'] = df2['promo2_since_year'].astype(str)+'-'+df2['promo2_since_week'].astype(str)\n",
    "df2['promo_since'] = df2['promo_since'].apply(lambda x: datetime.strptime(x+'-1','%Y-%W-%w')-timedelta(days=7))\n",
    "df2['promo_time_week'] = ((df2['date']-df2['promo_since'])/7).apply(lambda x: x.days).astype(int)\n",
    "\n",
    "#assortment\n",
    "df2['assortment'] = df2['assortment'].apply(lambda x: 'basic' if x=='a' else 'extra' if x=='b' else 'extended')\n",
    "\n",
    "#state holiday\n",
    "df2['state_holiday'] = df2['state_holiday'].apply(lambda x: 'public_holiday' if x=='a' else 'easter_holiday' if x=='b' else 'christmas' if x == 'c' else 'regular_day')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 FILTRAGEM DE VARIAVEIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "df3 = df2.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Filtragem das linhas\n",
    "    Linhas onde a loja estava fechada ou com vendas iguais a zero não serão úteis para a modelagem. Tiramos elas a seguir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "df3 = df3[df3['open'] != 0]\n",
    "df3 = df3[df3['sales'] > 0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Seleção das colunas\n",
    "    Agora também selecionamos as colunas que não agregam informação para o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "cols_drop = ['customers', 'open', 'promo_interval', 'month_map']\n",
    "df3 = df3.drop(cols_drop, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 ANALISE EXPLORATORIA DOS DADOS\n",
    "    Inicio da analise exploratória. Separamos novamente as variaveis em categoricas e numericas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "df4 = df3.copy()\n",
    "num_attributes = df4.select_dtypes(include=['int64', 'float64'])\n",
    "cat_attributes = df4.select_dtypes(exclude=['int64', 'float64', 'datetime64[ns]'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Analise univariada\n",
    "    Analise univariada nas variaveis resposta, númericas e categóricas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Variavel resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "#Variavel resposta\n",
    "sns.histplot(df4['sales'], kde=True, stat=\"density\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2Variaveis númericas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "num_attributes.hist(bins=25, figsize=(21,14));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "#Variaveis categoricas\n",
    "df4['state_holiday'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "#state holiday\n",
    "a=df4[df4['state_holiday'] != 'regular_day']\n",
    "fig = plt.subplots(3,2,figsize=(21,21))\n",
    "plt.subplot(3,2,1)\n",
    "sns.countplot(a,x='state_holiday')\n",
    "\n",
    "plt.subplot(3,2,2)\n",
    "sns.kdeplot(df4[df4['state_holiday'] == 'public_holiday']['sales'], label='public_holiday', fill=True);\n",
    "sns.kdeplot(df4[df4['state_holiday'] == 'easter_holiday']['sales'], label='public_holiday', fill=True);\n",
    "sns.kdeplot(df4[df4['state_holiday'] == 'christmas']['sales'], label='public_holiday', fill=True);\n",
    "\n",
    "#store_type\n",
    "plt.subplot(3,2,3)\n",
    "sns.countplot(df4,x='store_type')\n",
    "\n",
    "plt.subplot(3,2,4)\n",
    "sns.kdeplot(df4[df4['store_type'] == 'a']['sales'], label='a', fill=True);\n",
    "sns.kdeplot(df4[df4['store_type'] == 'b']['sales'], label='b', fill=True);\n",
    "sns.kdeplot(df4[df4['store_type'] == 'c']['sales'], label='c', fill=True);\n",
    "sns.kdeplot(df4[df4['store_type'] == 'd']['sales'], label='d', fill=True);\n",
    "\n",
    "#assortment\n",
    "plt.subplot(3,2,5)\n",
    "sns.countplot(df4,x='assortment')\n",
    "\n",
    "plt.subplot(3,2,6)\n",
    "sns.kdeplot(df4[df4['assortment'] == 'extended']['sales'], label='extended', fill=True);\n",
    "sns.kdeplot(df4[df4['assortment'] == 'basic']['sales'], label='basic', fill=True);\n",
    "sns.kdeplot(df4[df4['assortment'] == 'extra']['sales'], label='extra', fill=True);\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Analise bivariada\n",
    "    Aqui foi verificado se as hipoteses criadas anteriormente são verdadeiras ou falsas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H1 lojas com maior sortimentos deveriam vender mais\n",
    "\n",
    "***FALSA*** Na verdade lojas com maior sortimento vende menos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "aux1 = df4[['assortment','sales']].groupby('assortment').sum().reset_index()\n",
    "\n",
    "sns.barplot(x='assortment', y='sales', data=aux1);\n",
    "\n",
    "aux2 = df4[['year_week','assortment','sales']].groupby(['year_week', 'assortment']).sum().reset_index()\n",
    "aux2.pivot(index='year_week', columns='assortment', values='sales').plot()\n",
    "\n",
    "aux3 = aux2[aux2['assortment'] =='extra']\n",
    "aux3.pivot(index='year_week', columns='assortment', values='sales').plot()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H2 lojas com competidores mais próximos deveriam vender menos\n",
    "***FALSE*** Loja com competidores próximos vendem mais e não menos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "aux1 = df4[['competition_distance','sales']].groupby('competition_distance').sum().reset_index()\n",
    "\n",
    "bins = list(np.arange(0,20000,1000))\n",
    "\n",
    "fig = plt.subplots(figsize=(21,14));\n",
    "plt.subplot(2,2,1);\n",
    "sns.scatterplot(x='competition_distance', y='sales', data=aux1);\n",
    "\n",
    "plt.subplot(2,2,2);\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True);\n",
    "\n",
    "aux1['competition_distance_binned'] = pd.cut(aux1['competition_distance'], bins = bins)\n",
    "aux2 = aux1[['competition_distance_binned','sales']].groupby('competition_distance_binned').sum().reset_index()\n",
    "\n",
    "\n",
    "plt.subplot(2,2,(3,4));\n",
    "sns.barplot(x='competition_distance_binned', y='sales', data=aux2);\n",
    "plt.xticks(rotation=-45);\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H3 loja com competidores a mais tempo deveriam vender mais\n",
    "***FALSO*** loja com competidores a mais tempo vendem menos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "plt.subplots(2,2,figsize=(21,14))\n",
    "\n",
    "aux1 = df4[['competition_time_month','sales']].groupby('competition_time_month').sum().reset_index()\n",
    "aux2 = aux1[aux1['competition_time_month']<120]\n",
    "aux2 = aux2[aux2['competition_time_month'] != 0]\n",
    "\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "sns.regplot(x='competition_time_month', y='sales', data=aux2);\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True);\n",
    "\n",
    "plt.subplot(2,2,(3,4))\n",
    "sns.barplot(x='competition_time_month', y='sales', data=aux2);\n",
    "plt.xticks(rotation=-45);\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H4 Lojas com promoções ativas por mais tempo deveriam vender mais.\n",
    "\n",
    "***FALSO*** Ela vende regularmente por um periodo, porém tempo depois passa a cair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "aux1 = df4[['promo_time_week','sales']].groupby('promo_time_week').sum().reset_index()\n",
    "\n",
    "aux2 = aux1[aux1['promo_time_week']>0] #periodo extendido\n",
    "aux3 = aux1[aux1['promo_time_week']<0] #periodo regular\n",
    "\n",
    "fig = plt.subplots(2,3,figsize=(21,14))\n",
    "plt.subplot(2,3,1)\n",
    "sns.barplot(x='promo_time_week', y='sales', data=aux2);\n",
    "plt.xticks(rotation=-45);\n",
    "\n",
    "plt.subplot(2,3,4)\n",
    "sns.regplot(x='promo_time_week', y='sales', data=aux2);\n",
    "plt.xticks(rotation=-45);\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "sns.barplot(x='promo_time_week', y='sales', data=aux3);\n",
    "plt.xticks(rotation=-45);\n",
    "\n",
    "plt.subplot(2,3,5)\n",
    "sns.regplot(x='promo_time_week', y='sales', data=aux3);\n",
    "plt.xticks(rotation=-45);\n",
    "\n",
    "plt.subplot(2,3,(3,6))\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <s>H5 Lojas com mais dias de promoção deveriam vender mais</s>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H6 Lojas com mais promoções consecutivas deveriam vender mais\n",
    "\n",
    "***FALSO*** Lojas com mais promo consec, vendem menos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "df4[['promo','promo2','sales']].groupby(['promo', 'promo2']).sum().reset_index()\n",
    "\n",
    "\n",
    "aux1 = df4[df4['promo'] == 1]\n",
    "aux1 = aux1[aux1['promo2'] == 1][['year_week','sales']].groupby('year_week').sum().reset_index()\n",
    "ax = aux1.plot()\n",
    "\n",
    "aux2 = df4[df4['promo'] == 1]\n",
    "aux2 = aux2[aux2['promo2'] == 0][['year_week','sales']].groupby('year_week').sum().reset_index()\n",
    "aux2.plot(ax=ax)\n",
    "\n",
    "ax.legend(labels=['Tradicional e extendida', 'Extendida'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H7 Lojas abertas durante o feriado de Natal deveriam vender mais\n",
    "\n",
    "***FALSA*** Lojas abertas durante o feriado de natal vendem menos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "aux = df4[df4['state_holiday'] != 'regular_day']\n",
    "\n",
    "fig=plt.subplots(1,2,figsize=(21,7))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "aux1 = aux[['state_holiday','sales']].groupby('state_holiday').sum().reset_index()\n",
    "sns.barplot(x='state_holiday', y='sales', data = aux1);\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "aux2 = aux[['year', 'state_holiday', 'sales']].groupby(['year', 'state_holiday']).sum().reset_index()\n",
    "sns.barplot(x='year', y='sales', hue='state_holiday', data=aux2);\n",
    "\n",
    "#sns.countplot(x=aux['state_holiday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=aux['state_holiday'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H8 Lojas deveriam vendar mais ao longo dos anos\n",
    "\n",
    "***FALSO***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "aux1 = df4[['year','sales']].groupby('year').sum().reset_index()\n",
    "\n",
    "fig=plt.subplots(1,3,figsize=(21,7))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "sns.barplot(x='year', y='sales', data = aux1);\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.regplot(x='year', y='sales', data = aux1);\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H9 Lojas deveriam vender mais no segundo semestre do ano\n",
    "\n",
    "***FALSO***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "aux1 = df4[['month','sales']].groupby('month').sum().reset_index()\n",
    "\n",
    "fig=plt.subplots(1,3,figsize=(21,7))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "sns.barplot(x='month', y='sales', data = aux1);\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.regplot(x='month', y='sales', data = aux1);\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H11 Lojas deveriam vendar mais depois do dia 10 de cada mês\n",
    "\n",
    "***VERDADEIRO***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "aux1 = df4[['day','sales']].groupby('day').sum().reset_index()\n",
    "\n",
    "fig=plt.subplots(2,2,figsize=(21,14))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "sns.barplot(x='day', y='sales', data = aux1);\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "sns.regplot(x='day', y='sales', data = aux1);\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True);\n",
    "\n",
    "aux1['before_after'] = aux1['day'].apply(lambda x: 'before_10_days' if x<= 10 else 'after_10_days')\n",
    "aux2 = aux1[['before_after','sales']].groupby('before_after').sum().reset_index()\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "sns.barplot(x='before_after', y='sales', data = aux2);\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H12 Lojas deveriam vender menos aos finais de semana\n",
    "\n",
    "***Verdadeiro***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "aux1 = df4[['day_of_week','sales']].groupby('day_of_week').sum().reset_index()\n",
    "\n",
    "fig=plt.subplots(1,3,figsize=(21,7))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "sns.barplot(x='day_of_week', y='sales', data = aux1);\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.regplot(x='day_of_week', y='sales', data = aux1);\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H13 Lojas deveriam vender menos durante os feriados escolares.\n",
    "\n",
    "***Verdadeiro***, Exceto no mes de agosto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "aux1 = df4[['school_holiday','sales']].groupby('school_holiday').sum().reset_index()\n",
    "aux2 = df4[['month', 'school_holiday','sales']].groupby(['month', 'school_holiday']).sum().reset_index()\n",
    "\n",
    "fig=plt.subplots(1,3,figsize=(21,7))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "sns.barplot(x='school_holiday', y='sales', data = aux1);\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.barplot(x='month', y='sales', hue='school_holiday', data = aux2);\n",
    "\n",
    "'''plt.subplot(1,3,2)\n",
    "sns.regplot(x='school_holiday', y='sales', data = aux1);'''\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Analise multivariada\n",
    "    Analise multivariada, separado pelas variaveis categoricas e núméricas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,7))\n",
    "correlation = num_attributes.corr(method='pearson')\n",
    "sns.heatmap(correlation, annot=True);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "a = cat_attributes\n",
    "\n",
    "cramer_v(a['state_holiday'],a['store_type'])\n",
    "\n",
    "a1 = cramer_v(a['state_holiday'],a['state_holiday'])\n",
    "a2 = cramer_v(a['state_holiday'],a['store_type'])\n",
    "a3 = cramer_v(a['state_holiday'],a['assortment'])\n",
    "\n",
    "a4 = cramer_v(a['store_type'],a['state_holiday'])\n",
    "a5 = cramer_v(a['store_type'],a['store_type'])\n",
    "a6 = cramer_v(a['store_type'],a['assortment'])\n",
    "\n",
    "a7 = cramer_v(a['assortment'],a['state_holiday'])\n",
    "a8 = cramer_v(a['assortment'],a['store_type'])\n",
    "a9 = cramer_v(a['assortment'],a['assortment'])\n",
    "\n",
    "d = pd.DataFrame({'state_holiday': [a1,a2,a3],\n",
    "              'store_type': [a4,a5,a6],\n",
    "              'assortment': [a7,a8,a9]})\n",
    "\n",
    "d = d.set_index(d.columns)\n",
    "sns.heatmap(d, annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 DATA PREPARATION\n",
    "    Preparação dos dados. Aqui é tratado as colunas, normalizando-as quando necessário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "df5 = df4.copy()\n",
    "a = df5.select_dtypes(include=['int32', 'int64', 'float64'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Normalização\n",
    "\n",
    "### Nenhuma variavel com distribuição normal\n",
    "    A normalização funciona bem variaveis que possuem distribuição normal. Como visto na analise univariavel nenhum possue esse tipo de distribuição, portanto não foi aplicado\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Rescaling\n",
    "\n",
    "    Como não temos nenhuma variavel categórica, o método de reescaling foi aplicado. MinMaxScaler foi utilizado para colunas que não apresentam Outliers; quando existe outliers foi aplicado o RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "rs = RobustScaler()\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "#sns.boxplot(df5['competition_distance'])\n",
    "df5['competition_distance']=rs.fit_transform(a[['competition_distance']].values)\n",
    "pickle.dump(rs, open(os.path.join(PARA_DIR,'competition_distance_scaler.pkl'), 'wb'))\n",
    "\n",
    "#sns.boxplot(df5['competition_time_month'])\n",
    "df5['competition_time_month']=rs.fit_transform(a[['competition_time_month']].values)\n",
    "pickle.dump(rs, open(os.path.join(PARA_DIR,'competition_time_month_scaler.pkl'), 'wb'))\n",
    "\n",
    "\n",
    "#sns.boxplot(df5['promo_time_week'])\n",
    "df5['promo_time_week']=mms.fit_transform(a[['promo_time_week']].values)\n",
    "pickle.dump(mms, open(os.path.join(PARA_DIR,'promo_time_week_scaler.pkl'), 'wb'))\n",
    "\n",
    "#sns.boxplot(df5['year'])\n",
    "df5['year']=mms.fit_transform(a[['year']].values)\n",
    "pickle.dump(mms, open(os.path.join(PARA_DIR,'year_scaler.pkl'), 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Transformação\n",
    "\n",
    "    Nas variaveis categoricas foi aplicado técnicas de transformação, como o onehotencoding, label encoding e ordinal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "#state holiday - onehotencoding\n",
    "df5 = pd.get_dummies(df5,prefix=['state_holiday'], columns=['state_holiday'],dtype=float)\n",
    "\n",
    "#store_type  Label encoding\n",
    "le = LabelEncoder()\n",
    "df5['store_type'] = le.fit_transform(df5['store_type'])\n",
    "pickle.dump(le, open(os.path.join(PARA_DIR,'store_type_scaler.pkl'), 'wb'))\n",
    "\n",
    "#store_type  ordinal encoding\n",
    "assortment_dict = {'basic': 1, 'extra': 2, 'extended': 3}\n",
    "df5['assortment'] = df5['assortment'].map(assortment_dict)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Para variaveis ciclicas (ano, meses, semanas e dias) foi utilizado uma técnica de escrevelas em sua decomposição em seno e cosseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "#Transformação em variaveis ciclicas\n",
    "\n",
    "df5['month_sin'] = df5['month'].apply(lambda x: np.sin(x*(2*np.pi/12)))\n",
    "df5['month_cos'] = df5['month'].apply(lambda x: np.cos(x*(2*np.pi/12)))\n",
    "\n",
    "df5['day_sin'] = df5['day'].apply(lambda x: np.sin(x*(2*np.pi/30)))\n",
    "df5['day_cos'] = df5['day'].apply(lambda x: np.cos(x*(2*np.pi/30)))\n",
    "\n",
    "df5['week_of_year_sin'] = df5['week_of_year'].apply(lambda x: np.sin(x*(2*np.pi/52)))\n",
    "df5['week_of_year_cos'] = df5['week_of_year'].apply(lambda x: np.cos(x*(2*np.pi/52)))\n",
    "\n",
    "df5['day_of_week_sin'] = df5['day_of_week'].apply(lambda x: np.sin(x*(2*np.pi/7)))\n",
    "df5['day_of_week_cos'] = df5['day_of_week'].apply(lambda x: np.cos(x*(2*np.pi/7)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 FEATURE SELECTION\n",
    "    Nessa seção vamos selecionar as varivaveis de interesse para o modelo. Algumas variaveis que foram utilizadas para definir outras serão automaticamente descartadas (para evitar dependencia linear no modelo). As demais serão escolhidas pelo método do algoritmo Boruta."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Preparação dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "df6 = df5.copy()\n",
    "cols_drop = ['week_of_year', 'day', 'month', 'day_of_week', 'promo_since', 'competition_since', 'year_week']\n",
    "df6 = df6.drop(cols_drop, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Separação de treino e teste para utilizar no algoritmo Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train = df6[df6['date']<'2015-06-19']\n",
    "y_train = X_train['sales']\n",
    "\n",
    "X_teste = df6[df6['date']>='2015-06-19']\n",
    "y_teste = X_teste['sales']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Aplicação Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_n = X_train.drop(['date','sales'], axis=1).values\n",
    "y_train_n = y_train.values.ravel()\n",
    "\n",
    "rf = RandomForestRegressor(n_jobs=-1)\n",
    "\n",
    "boruta = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=43).fit(X_train_n, y_train_n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Armazenamento das variaveis classificadas como boa pelo boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_selected = boruta.support_.tolist()\n",
    "\n",
    "X_train_fs = X_train.drop(['date', 'sales'], axis=1)\n",
    "\n",
    "cols_selected_boruta = X_train_fs.iloc[:, cols_selected].columns.to_list()\n",
    "\n",
    "cols_not_selected_boruta = list(np.setdiff1d(X_train_fs.columns, cols_selected_boruta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_selected_boruta_bau = ['store',\n",
    "                            'promo',\n",
    "                            'store_type',\n",
    "                            'assortment',\n",
    "                            'competition_distance',\n",
    "                            'competition_open_since_month',\n",
    "                            'competition_open_since_year',\n",
    "                            'promo2',\n",
    "                            'promo2_since_week',\n",
    "                            'promo2_since_year',\n",
    "                            'competition_time_month',\n",
    "                            'promo_time_week',\n",
    "                            'month_sin',\n",
    "                            'month_cos',\n",
    "                            'day_sin',\n",
    "                            'day_cos',\n",
    "                            'day_of_week_sin',\n",
    "                            'day_of_week_cos',\n",
    "                            'week_of_year_cos',\n",
    "                            'week_of_year_sin']\n",
    "\n",
    "feat_to_add = ['date', 'sales']\n",
    "\n",
    "cols_selected_boruta_full = cols_selected_boruta_bau.copy()\n",
    "cols_selected_boruta_full.extend(feat_to_add)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 MACHINE LEARGING MODEL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Aplicação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train[cols_selected_boruta_bau]\n",
    "x_teste = X_teste[cols_selected_boruta_bau]\n",
    "x_training = X_train[cols_selected_boruta_full]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = x_teste.copy()\n",
    "aux1['sales'] = y_teste.copy()\n",
    "\n",
    "#predict\n",
    "aux2 = aux1[['store', 'sales']].groupby('store').mean().reset_index().rename(columns={'sales': 'predictions'})\n",
    "aux1 = pd.merge(aux1, aux2, how='left', on='store')\n",
    "yhat_baseline = aux1['predictions']\n",
    "\n",
    "baseline_result = ml_error('Average Model', y_teste, yhat_baseline)\n",
    "baseline_result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression().fit(x_train, y_train)\n",
    "\n",
    "yhat_lr = lr.predict(x_teste)\n",
    "\n",
    "lr_result = ml_error( 'LinearRegression', y_teste, yhat_lr)\n",
    "lr_result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression - cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr = LinearRegression()\n",
    "lr_result_cv = cross_validation(x_training, 5, 'Linear Regression', lr, verbose=False)\n",
    "lr_result_cv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression regularizado LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrr = Lasso().fit(x_train, y_train)\n",
    "\n",
    "yhat_lrr = lrr.predict(x_teste)\n",
    "\n",
    "lrr_result = ml_error( 'LinearRegression - LASSO', y_teste, yhat_lrr)\n",
    "lrr_result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression reg - cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrr_result_cv = cross_validation(x_training, 5, 'Lasso', lrr, verbose=True)\n",
    "lrr_result_cv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=43).fit(x_train, y_train)\n",
    "\n",
    "yhat_rf = rf.predict(x_teste)\n",
    "\n",
    "rf_result = ml_error( 'RFR', y_teste, yhat_rf)\n",
    "rf_result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest reg - cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_result_cv = cross_validation(x_training, 5, 'Random Forest Regressor', rf, verbose=True)\n",
    "rf_result_cv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = xgb.XGBRegressor(objective='reg:squarederror').fit(x_train, y_train)\n",
    "\n",
    "yhat_xgb = model_xgb.predict(x_teste)\n",
    "\n",
    "xgb_result = ml_error( 'xgboost', y_teste, yhat_xgb)\n",
    "xgb_result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Reg - cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_result_cv = cross_validation(x_training, 5, 'XGB Regressor', model_xgb, verbose=True)\n",
    "xgb_result_cv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparação de performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_result = pd.concat([baseline_result, lr_result, lrr_result, rf_result, xgb_result])\n",
    "modelling_result.sort_values('RMSE')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_result_cv = pd.concat([lr_result_cv, lrr_result_cv, rf_result_cv, xgb_result_cv])\n",
    "modelling_result_cv.sort_values('RMSE CV')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.0 HYPERPARAMETER FINE TUNING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'eta': [0.1, 0.2, 0.3],\n",
    "         'max_depth': [4, 6, 8],\n",
    "         'subsample': [1, 0.8, 0.6],\n",
    "         'colsample_bytree': [1, 0.8, 0.6],\n",
    "         'min_child_weight': [1, 6, 11]}\n",
    "\n",
    "MAX_EVAL = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = pd.DataFrame()\n",
    "\n",
    "for i in range (MAX_EVAL):\n",
    "    \n",
    "    hp = {k: random.sample(v, 1)[0] for k, v in param.items()}\n",
    "    print(hp)\n",
    "    \n",
    "    model_xgb = xgb.XGBRegressor(objective='reg:squarederror',\n",
    "                                eta = hp['eta'],\n",
    "                                max_depth = hp['max_depth'],\n",
    "                                subsample = hp['subsample'],\n",
    "                                colsample_bytree=hp['colsample_bytree'],\n",
    "                                min_child_weight=hp['min_child_weight'])\n",
    "    \n",
    "\n",
    "    result = cross_validation( x_training, 5, 'XGB', model_xgb, verbose=True)\n",
    "    final_result = pd.concat([final_result, result])\n",
    "\n",
    "final_result\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_tuned = {'eta': 0.3,\n",
    "               'max_depth': 8,\n",
    "               'subsample': 1,\n",
    "               'colsample_bytree': 1,\n",
    "               'min_child_weight': 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb_tuned = xgb.XGBRegressor(objective='reg:squarederror',\n",
    "                                   n_estimators = 3000,\n",
    "                                   eta = param_tuned['eta'],\n",
    "                                   max_depth = param_tuned['max_depth'],\n",
    "                                   subsample = param_tuned['subsample'],\n",
    "                                   colsample_bytree=param_tuned['colsample_bytree'],\n",
    "                                   min_child_weight=param_tuned['min_child_weight']).fit(x_train, y_train)\n",
    "    \n",
    "yhat_tuned = model_xgb_tuned.predict(x_teste)\n",
    "\n",
    "result_tuned = ml_error('XGB', y_teste, yhat_tuned)\n",
    "result_tuned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.0 TRADUÇÃO E INTERPRETAÇÃO DO ERRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = X_teste[cols_selected_boruta_full].copy()\n",
    "df9['predictions'] = yhat_tuned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9_1 = df9[['store', 'predictions']].groupby('store').sum().reset_index()\n",
    "\n",
    "df9_aux1 = df9[['store', 'sales', 'predictions']].groupby('store').apply(lambda x: mean_absolute_error(x['sales'], x['predictions'])).reset_index().rename(columns={0: 'MAE'})\n",
    "df9_aux2 = df9[['store', 'sales', 'predictions']].groupby('store').apply(lambda x: mean_absolute_percentage_error(x['sales'], x['predictions'])).reset_index().rename(columns={0: 'MAPE'})\n",
    "df9_aux3 = pd.merge(df9_aux1, df9_aux2, how='inner', on='store')\n",
    "\n",
    "df9_2 = pd.merge(df9_1, df9_aux3, how='inner', on='store')\n",
    "df9_2\n",
    "\n",
    "#Pior cenario\n",
    "df9_2['worst_scenario'] = df9_2['predictions']-df9_2['MAE'] \n",
    "df9_2['best_scenario'] = df9_2['predictions']+df9_2['MAE'] \n",
    "\n",
    "df9_2 = df9_2[['store', 'predictions', 'worst_scenario', 'best_scenario', 'MAE', 'MAPE']]\n",
    "df9_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9_2.sort_values('MAPE', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='store', y='MAPE', data=df9_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9_3 = df9_2[['predictions', 'worst_scenario', 'best_scenario']].apply(lambda x: np.sum(x), axis=0).reset_index().rename(columns={'index': 'Scenarios', 0: 'Values'})\n",
    "df9_3['Values'] = df9_3['Values'].map('${:,.2f}'.format)\n",
    "df9_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9['error'] = df9['sales']- df9['predictions']\n",
    "df9['error_rate'] = df9['predictions']/df9['sales']\n",
    "\n",
    "fig = plt.subplots(2,2,figsize=(21,14))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "sns.lineplot(x='date', y='sales', data=df9, label='SALES')\n",
    "sns.lineplot(x='date', y='predictions', data=df9, label='PREDICTIONS')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "sns.lineplot(x='date', y='error_rate', data=df9)\n",
    "\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "sns.histplot(df9['error'], kde=True, stat=\"density\");\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "sns.scatterplot(x=df9['predictions'], y=df9['error'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.0 DEPLOY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle.dump(model_xgb_tuned, open(os.path.join(PARA_DIR,'model_rossman.pkl'), 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import inflection \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "class Rossmann (object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.home_path = '/home/mayconr/repos/ComunidadeDS/dsproducao/rossmann_project/'\n",
    "        self.competition_time_month_scaler  = pickle.load(open(self.home_path + 'parameter/competition_time_month_scaler.pkl', 'rb'))\n",
    "        self.competition_distance_scaler    = pickle.load(open(self.home_path + 'parameter/competition_distance_scaler.pkl', 'rb'))\n",
    "        self.promo_time_week_scaler         = pickle.load(open(self.home_path + 'parameter/promo_time_week_scaler.pkl', 'rb'))\n",
    "        self.store_type_scaler              = pickle.load(open(self.home_path + 'parameter/store_type_scaler.pkl', 'rb'))\n",
    "        self.year_scaler                    = pickle.load(open(self.home_path + 'parameter/year_scaler.pkl', 'rb'))\n",
    "        \n",
    "    \n",
    "    def data_cleaning(self, df1):\n",
    "     \n",
    "        cols_old = ['Store', 'DayOfWeek', 'Date', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment', 'CompetitionDistance',\n",
    "                    'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval']\n",
    "\n",
    "        snakecase = lambda x: inflection.underscore(x)\n",
    "        cols_new = list(map(snakecase, cols_old))\n",
    "\n",
    "        #rename\n",
    "        df1.columns = cols_new\n",
    "\n",
    "        df1['date'] = pd.to_datetime(df1['date'])\n",
    "        \n",
    "        # competition_distance\n",
    "        df1['competition_distance'].fillna(200000.0, inplace=True)\n",
    "\n",
    "        # competition_open_since_month\n",
    "        df1['competition_open_since_month'] = df1.apply(lambda x: x['date'].month if math.isnan(x['competition_open_since_month']) else x['competition_open_since_month'], axis=1)\n",
    "\n",
    "        # competition_open_since_year\n",
    "        df1['competition_open_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['competition_open_since_year']) else x['competition_open_since_year'], axis=1)\n",
    "\n",
    "        # promo2_since_week\n",
    "        df1['promo2_since_week'] = df1.apply(lambda x: x['date'].week if math.isnan(x['promo2_since_week']) else x['promo2_since_week'], axis=1)\n",
    "\n",
    "        # promo2_since_year\n",
    "        df1['promo2_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['promo2_since_year']) else x['promo2_since_year'], axis=1)\n",
    "\n",
    "        # promo_interval\n",
    "        df1['promo_interval'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "        month_map = {1: 'Jan', 2: 'Fev', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', 7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'}\n",
    "        df1['month_map'] = df1['date'].dt.month.map(month_map)\n",
    "        df1['is_promo'] = df1.apply(is_promo, axis=1)\n",
    "\n",
    "        df1['competition_open_since_month'] = df1['competition_open_since_month'].astype(int)\n",
    "        df1['competition_open_since_year'] = df1['competition_open_since_year'].astype(int)\n",
    "        df1['promo2_since_week'] = df1['promo2_since_week'].astype(int)\n",
    "        df1['promo2_since_year'] = df1['promo2_since_year'].astype(int)\n",
    "        \n",
    "        return df1\n",
    "    \n",
    "    \n",
    "    def feature_engineering(self, df2):\n",
    "        #Year\n",
    "        df2['year'] = df2['date'].dt.year\n",
    "\n",
    "        #Month\n",
    "        df2['month'] = df2['date'].dt.month\n",
    "\n",
    "        #Day\n",
    "        df2['day'] = df2['date'].dt.day\n",
    "\n",
    "        #Week of Year\n",
    "        df2['week_of_year'] = df2['date'].dt.isocalendar().week\n",
    "\n",
    "        #Year week\n",
    "        df2['year_week'] = df2['date'].dt.strftime('%Y-%W')\n",
    "\n",
    "\n",
    "        #competition since\n",
    "        df2['competition_since'] = df2.apply(lambda x: datetime(year=x['competition_open_since_year'], month=x['competition_open_since_month'],day=1),axis=1)\n",
    "        df2['competition_time_month'] = ((df2['date']-df2['competition_since'])/30).apply(lambda x: x.days).astype(int)\n",
    "\n",
    "        #Promo since\n",
    "        df2['promo_since'] = df2['promo2_since_year'].astype(str)+'-'+df2['promo2_since_week'].astype(str)\n",
    "        df2['promo_since'] = df2['promo_since'].apply(lambda x: datetime.strptime(x+'-1','%Y-%W-%w')-timedelta(days=7))\n",
    "        df2['promo_time_week'] = ((df2['date']-df2['promo_since'])/7).apply(lambda x: x.days).astype(int)\n",
    "\n",
    "        #assortment\n",
    "        df2['assortment'] = df2['assortment'].apply(lambda x: 'basic' if x=='a' else 'extra' if x=='b' else 'extended')\n",
    "\n",
    "        #state holiday\n",
    "        df2['state_holiday'] = df2['state_holiday'].apply(lambda x: 'public_holiday' if x=='a' else 'easter_holiday' if x=='b' else 'christmas' if x == 'c' else 'regular_day')\n",
    "\n",
    "        ## 3.1 Filtragem das linhas\n",
    "\n",
    "        df2 = df2[df2['open'] != 0]\n",
    "        ## 3.2 Seleção das colunas\n",
    "\n",
    "        cols_drop = ['open', 'promo_interval', 'month_map']\n",
    "        df2 = df2.drop(cols_drop, axis=1)\n",
    "        \n",
    "        return df2\n",
    "    \n",
    "    def data_preparation(self,df5):\n",
    "    \n",
    "\n",
    "        a = df5.select_dtypes(include=['int32', 'int64', 'float64'])\n",
    "\n",
    "        df5['competition_distance']= self.competition_distance_scaler.fit_transform(a[['competition_distance']].values)\n",
    "\n",
    "        df5['competition_time_month']=self.competition_time_month_scaler.fit_transform(a[['competition_time_month']].values)\n",
    "        \n",
    "        df5['promo_time_week']=self.promo_time_week_scaler.fit_transform(a[['promo_time_week']].values)\n",
    "\n",
    "        df5['year']=self.year_scaler.fit_transform(a[['year']].values)\n",
    "\n",
    "\n",
    "        #state holiday - onehotencoding\n",
    "        df5 = pd.get_dummies(df5,prefix=['state_holiday'], columns=['state_holiday'],dtype=float)\n",
    "\n",
    "        df5['store_type'] = self.store_type_scaler.fit_transform(df5['store_type'])\n",
    "\n",
    "\n",
    "        #store_type  ordinal encoding\n",
    "        assortment_dict = {'basic': 1, 'extra': 2, 'extended': 3}\n",
    "        df5['assortment'] = df5['assortment'].map(assortment_dict)\n",
    "\n",
    "        df5['month_sin'] = df5['month'].apply(lambda x: np.sin(x*(2*np.pi/12)))\n",
    "        df5['month_cos'] = df5['month'].apply(lambda x: np.cos(x*(2*np.pi/12)))\n",
    "\n",
    "        df5['day_sin'] = df5['day'].apply(lambda x: np.sin(x*(2*np.pi/30)))\n",
    "        df5['day_cos'] = df5['day'].apply(lambda x: np.cos(x*(2*np.pi/30)))\n",
    "\n",
    "        df5['week_of_year_sin'] = df5['week_of_year'].apply(lambda x: np.sin(x*(2*np.pi/52)))\n",
    "        df5['week_of_year_cos'] = df5['week_of_year'].apply(lambda x: np.cos(x*(2*np.pi/52)))\n",
    "\n",
    "        df5['day_of_week_sin'] = df5['day_of_week'].apply(lambda x: np.sin(x*(2*np.pi/7)))\n",
    "        df5['day_of_week_cos'] = df5['day_of_week'].apply(lambda x: np.cos(x*(2*np.pi/7)))\n",
    "        \n",
    "        cols_selected = ['store','promo','store_type','assortment','competition_distance',\n",
    "                         'competition_open_since_month','competition_open_since_year','promo2',\n",
    "                         'promo2_since_week','promo2_since_year','competition_time_month',\n",
    "                         'promo_time_week','month_sin','month_cos','day_sin','day_cos',\n",
    "                         'day_of_week_sin','day_of_week_cos','week_of_year_cos','week_of_year_sin']\n",
    "\n",
    "        \n",
    "        return df5[cols_selected]\n",
    "    \n",
    "    def get_prediction(self, model, original_data, test_data):\n",
    "        pred=model.predict(test_data)\n",
    "        original_data['prediction'] = pred\n",
    "        return original_data.to_json(orient='records', date_format='iso')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from flask import Flask, request, Response\n",
    "from rossmann.Rossmann import Rossmann\n",
    "\n",
    "#load model\n",
    "\n",
    "model = pickle.load(open('/home/mayconr/repos/ComunidadeDS/dsproducao/rossmann_project/model/model_rossman.pkl', 'rb'))\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/rossmann/predict', methods=['POST'])\n",
    "def rossmann_predict():\n",
    "    test_json = request.get_json()\n",
    "    \n",
    "    if test_json: #there is data\n",
    "        if isinstance(test_json, dict): #unique example\n",
    "            teste_raw = pd.DataFrame(test_json, index=[0])\n",
    "        else: #Multiple example\n",
    "            teste_raw = pd.DataFrame(test_json, columns= test_json[0].keys())\n",
    "            \n",
    "        # instantiate rossmann class\n",
    "        pipeline = Rossmann()\n",
    "        \n",
    "        #data cleaning\n",
    "        df1 = pipeline.data_cleaning(teste_raw)\n",
    "        \n",
    "        #data engineering\n",
    "        df2 = pipeline.feature_engineering(df1)\n",
    "        \n",
    "        #data preparation\n",
    "        df3 = pipeline.data_preparation(df2)\n",
    "        \n",
    "        #prediction\n",
    "        df_response = pipeline.get_prediction(model, teste_raw, df3)\n",
    "        \n",
    "        return df_response\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        return Response('{}', status=200, mimetype='application/json')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run('0.0.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10 = pd.read_csv('/home/mayconr/repos/ComunidadeDS/dsproducao/rossmann_project/data/test.csv')\n",
    "\n",
    "df_test = pd.merge(df10, df_store_raw, how= 'left', on= 'Store')\n",
    "\n",
    "df_test = df_test[df_test['Store'].isin([22, 23, 24, 25])]\n",
    "\n",
    "df_test = df_test[df_test['Open']!=0]\n",
    "df_test = df_test[~df_test['Open'].isnull()]\n",
    "\n",
    "df_test = df_test.drop('Id', axis=1)\n",
    "\n",
    "data = df_test.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = 'http://0.0.0.0:5000/rossmann/predict'\n",
    "\n",
    "header = {'Content-type': 'application/json'} \n",
    "\n",
    "#data = data\n",
    "\n",
    "\n",
    "\n",
    "r = requests.post(url, data=json.dumps(data), headers=header)\n",
    "print('Status Code{}'.format(r.status_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>promo</th>\n",
       "      <th>state_holiday</th>\n",
       "      <th>school_holiday</th>\n",
       "      <th>store_type</th>\n",
       "      <th>assortment</th>\n",
       "      <th>competition_distance</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>year_week</th>\n",
       "      <th>competition_since</th>\n",
       "      <th>competition_time_month</th>\n",
       "      <th>promo_since</th>\n",
       "      <th>promo_time_week</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-09-17T00:00:00.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>regular_day</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>basic</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2015</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>38</td>\n",
       "      <td>2015-37</td>\n",
       "      <td>2015-09-01T00:00:00.000</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-05-21T00:00:00.000</td>\n",
       "      <td>173</td>\n",
       "      <td>8796.821289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-09-17T00:00:00.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>regular_day</td>\n",
       "      <td>0</td>\n",
       "      <td>d</td>\n",
       "      <td>basic</td>\n",
       "      <td>4060.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2015</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>38</td>\n",
       "      <td>2015-37</td>\n",
       "      <td>2005-08-01T00:00:00.000</td>\n",
       "      <td>123</td>\n",
       "      <td>2015-09-14T00:00:00.000</td>\n",
       "      <td>0</td>\n",
       "      <td>10667.041016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-09-17T00:00:00.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>regular_day</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>extended</td>\n",
       "      <td>4590.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2015</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>38</td>\n",
       "      <td>2015-37</td>\n",
       "      <td>2000-03-01T00:00:00.000</td>\n",
       "      <td>189</td>\n",
       "      <td>2011-09-26T00:00:00.000</td>\n",
       "      <td>207</td>\n",
       "      <td>8865.985352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>2015-09-17T00:00:00.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>regular_day</td>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "      <td>basic</td>\n",
       "      <td>430.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2015</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>38</td>\n",
       "      <td>2015-37</td>\n",
       "      <td>2003-04-01T00:00:00.000</td>\n",
       "      <td>151</td>\n",
       "      <td>2015-09-14T00:00:00.000</td>\n",
       "      <td>0</td>\n",
       "      <td>13095.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>2015-09-16T00:00:00.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>regular_day</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>basic</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2015</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>38</td>\n",
       "      <td>2015-37</td>\n",
       "      <td>2015-09-01T00:00:00.000</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-05-21T00:00:00.000</td>\n",
       "      <td>173</td>\n",
       "      <td>8490.807617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-08-03T00:00:00.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>regular_day</td>\n",
       "      <td>1</td>\n",
       "      <td>c</td>\n",
       "      <td>basic</td>\n",
       "      <td>430.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>2015-31</td>\n",
       "      <td>2003-04-01T00:00:00.000</td>\n",
       "      <td>150</td>\n",
       "      <td>2015-08-03T00:00:00.000</td>\n",
       "      <td>0</td>\n",
       "      <td>14403.689453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>2015-08-01T00:00:00.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>regular_day</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>basic</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>2015-30</td>\n",
       "      <td>2015-08-01T00:00:00.000</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-05-21T00:00:00.000</td>\n",
       "      <td>166</td>\n",
       "      <td>5899.893066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>2015-08-01T00:00:00.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>regular_day</td>\n",
       "      <td>0</td>\n",
       "      <td>d</td>\n",
       "      <td>basic</td>\n",
       "      <td>4060.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>2015-30</td>\n",
       "      <td>2005-08-01T00:00:00.000</td>\n",
       "      <td>121</td>\n",
       "      <td>2015-07-27T00:00:00.000</td>\n",
       "      <td>0</td>\n",
       "      <td>7222.688477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>2015-08-01T00:00:00.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>regular_day</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>extended</td>\n",
       "      <td>4590.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>2015-30</td>\n",
       "      <td>2000-03-01T00:00:00.000</td>\n",
       "      <td>187</td>\n",
       "      <td>2011-09-26T00:00:00.000</td>\n",
       "      <td>200</td>\n",
       "      <td>6072.159668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>2015-08-01T00:00:00.000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>regular_day</td>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "      <td>basic</td>\n",
       "      <td>430.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2015</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>2015-30</td>\n",
       "      <td>2003-04-01T00:00:00.000</td>\n",
       "      <td>150</td>\n",
       "      <td>2015-07-27T00:00:00.000</td>\n",
       "      <td>0</td>\n",
       "      <td>12624.048828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>163 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     store  day_of_week                     date  open  promo state_holiday  \\\n",
       "0       22            4  2015-09-17T00:00:00.000   1.0      1   regular_day   \n",
       "1       23            4  2015-09-17T00:00:00.000   1.0      1   regular_day   \n",
       "2       24            4  2015-09-17T00:00:00.000   1.0      1   regular_day   \n",
       "3       25            4  2015-09-17T00:00:00.000   1.0      1   regular_day   \n",
       "4       22            3  2015-09-16T00:00:00.000   1.0      1   regular_day   \n",
       "..     ...          ...                      ...   ...    ...           ...   \n",
       "158     25            1  2015-08-03T00:00:00.000   1.0      1   regular_day   \n",
       "159     22            6  2015-08-01T00:00:00.000   1.0      0   regular_day   \n",
       "160     23            6  2015-08-01T00:00:00.000   1.0      0   regular_day   \n",
       "161     24            6  2015-08-01T00:00:00.000   1.0      0   regular_day   \n",
       "162     25            6  2015-08-01T00:00:00.000   1.0      0   regular_day   \n",
       "\n",
       "     school_holiday store_type assortment  competition_distance  ...  year  \\\n",
       "0                 0          a      basic                1040.0  ...  2015   \n",
       "1                 0          d      basic                4060.0  ...  2015   \n",
       "2                 0          a   extended                4590.0  ...  2015   \n",
       "3                 0          c      basic                 430.0  ...  2015   \n",
       "4                 0          a      basic                1040.0  ...  2015   \n",
       "..              ...        ...        ...                   ...  ...   ...   \n",
       "158               1          c      basic                 430.0  ...  2015   \n",
       "159               0          a      basic                1040.0  ...  2015   \n",
       "160               0          d      basic                4060.0  ...  2015   \n",
       "161               0          a   extended                4590.0  ...  2015   \n",
       "162               0          c      basic                 430.0  ...  2015   \n",
       "\n",
       "     month  day  week_of_year  year_week        competition_since  \\\n",
       "0        9   17            38    2015-37  2015-09-01T00:00:00.000   \n",
       "1        9   17            38    2015-37  2005-08-01T00:00:00.000   \n",
       "2        9   17            38    2015-37  2000-03-01T00:00:00.000   \n",
       "3        9   17            38    2015-37  2003-04-01T00:00:00.000   \n",
       "4        9   16            38    2015-37  2015-09-01T00:00:00.000   \n",
       "..     ...  ...           ...        ...                      ...   \n",
       "158      8    3            32    2015-31  2003-04-01T00:00:00.000   \n",
       "159      8    1            31    2015-30  2015-08-01T00:00:00.000   \n",
       "160      8    1            31    2015-30  2005-08-01T00:00:00.000   \n",
       "161      8    1            31    2015-30  2000-03-01T00:00:00.000   \n",
       "162      8    1            31    2015-30  2003-04-01T00:00:00.000   \n",
       "\n",
       "    competition_time_month              promo_since  promo_time_week  \\\n",
       "0                        0  2012-05-21T00:00:00.000              173   \n",
       "1                      123  2015-09-14T00:00:00.000                0   \n",
       "2                      189  2011-09-26T00:00:00.000              207   \n",
       "3                      151  2015-09-14T00:00:00.000                0   \n",
       "4                        0  2012-05-21T00:00:00.000              173   \n",
       "..                     ...                      ...              ...   \n",
       "158                    150  2015-08-03T00:00:00.000                0   \n",
       "159                      0  2012-05-21T00:00:00.000              166   \n",
       "160                    121  2015-07-27T00:00:00.000                0   \n",
       "161                    187  2011-09-26T00:00:00.000              200   \n",
       "162                    150  2015-07-27T00:00:00.000                0   \n",
       "\n",
       "       prediction  \n",
       "0     8796.821289  \n",
       "1    10667.041016  \n",
       "2     8865.985352  \n",
       "3    13095.437500  \n",
       "4     8490.807617  \n",
       "..            ...  \n",
       "158  14403.689453  \n",
       "159   5899.893066  \n",
       "160   7222.688477  \n",
       "161   6072.159668  \n",
       "162  12624.048828  \n",
       "\n",
       "[163 rows x 28 columns]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = pd.DataFrame(r.json(), columns=r.json()[0].keys())\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sotre number 22 will sell $248,822.87 in the next 6 weeks\n",
      "Sotre number 23 will sell $383,744.36 in the next 6 weeks\n",
      "Sotre number 24 will sell $342,771.86 in the next 6 weeks\n",
      "Sotre number 25 will sell $524,725.90 in the next 6 weeks\n"
     ]
    }
   ],
   "source": [
    "d2 =d1[['store', 'prediction']].groupby('store').sum().reset_index()\n",
    "\n",
    "for i in range(len(d2)):\n",
    "    print('Sotre number {} will sell ${:,.2f} in the next 6 weeks'. format(d2.loc[i,'store'], d2.loc[i,'prediction']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rossman-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
